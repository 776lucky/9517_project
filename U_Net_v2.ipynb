{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cpu\n",
      "\n",
      "🚀 Epoch 1/20 开始\n",
      "  🌀 Batch   1/45 | Loss: 0.7203 | IoU: 0.0206\n",
      "    🔍 Pred min/max: -4.8719 ~ 8.0099\n",
      "    📊 Pred after sigmoid: 0.0076 ~ 0.9997\n",
      "  🌀 Batch   2/45 | Loss: 0.7032 | IoU: 0.0215\n",
      "    🔍 Pred min/max: -2.6776 ~ 9.1597\n",
      "    📊 Pred after sigmoid: 0.0643 ~ 0.9999\n",
      "  🌀 Batch   3/45 | Loss: 0.6942 | IoU: 0.0411\n",
      "    🔍 Pred min/max: -1.9927 ~ 7.3746\n",
      "    📊 Pred after sigmoid: 0.1200 ~ 0.9994\n",
      "  🌀 Batch   4/45 | Loss: 0.7086 | IoU: 0.0264\n",
      "    🔍 Pred min/max: -2.2853 ~ 8.0763\n",
      "    📊 Pred after sigmoid: 0.0923 ~ 0.9997\n",
      "  🌀 Batch   5/45 | Loss: 0.6686 | IoU: 0.0697\n",
      "    🔍 Pred min/max: -1.8352 ~ 10.3818\n",
      "    📊 Pred after sigmoid: 0.1376 ~ 1.0000\n",
      "  🌀 Batch   6/45 | Loss: 0.6735 | IoU: 0.0887\n",
      "    🔍 Pred min/max: -2.1310 ~ 10.6483\n",
      "    📊 Pred after sigmoid: 0.1061 ~ 1.0000\n",
      "  🌀 Batch   7/45 | Loss: 0.6844 | IoU: 0.0424\n",
      "    🔍 Pred min/max: -1.7180 ~ 16.2613\n",
      "    📊 Pred after sigmoid: 0.1521 ~ 1.0000\n",
      "  🌀 Batch   8/45 | Loss: 0.6478 | IoU: 0.0546\n",
      "    🔍 Pred min/max: -2.0235 ~ 17.9903\n",
      "    📊 Pred after sigmoid: 0.1168 ~ 1.0000\n",
      "  🌀 Batch   9/45 | Loss: 0.6499 | IoU: 0.0621\n",
      "    🔍 Pred min/max: -1.4083 ~ 26.1888\n",
      "    📊 Pred after sigmoid: 0.1965 ~ 1.0000\n",
      "  🌀 Batch  10/45 | Loss: 0.6529 | IoU: 0.0373\n",
      "    🔍 Pred min/max: -1.5854 ~ 30.2232\n",
      "    📊 Pred after sigmoid: 0.1700 ~ 1.0000\n",
      "  🌀 Batch  11/45 | Loss: 0.6741 | IoU: 0.0613\n",
      "    🔍 Pred min/max: -1.9567 ~ 26.3992\n",
      "    📊 Pred after sigmoid: 0.1238 ~ 1.0000\n",
      "  🌀 Batch  12/45 | Loss: 0.6392 | IoU: 0.0808\n",
      "    🔍 Pred min/max: -1.2418 ~ 30.7180\n",
      "    📊 Pred after sigmoid: 0.2241 ~ 1.0000\n",
      "  🌀 Batch  13/45 | Loss: 0.6499 | IoU: 0.0984\n",
      "    🔍 Pred min/max: -1.0887 ~ 22.3001\n",
      "    📊 Pred after sigmoid: 0.2519 ~ 1.0000\n",
      "  🌀 Batch  14/45 | Loss: 0.6544 | IoU: 0.0700\n",
      "    🔍 Pred min/max: -0.9687 ~ 25.7857\n",
      "    📊 Pred after sigmoid: 0.2751 ~ 1.0000\n",
      "  🌀 Batch  15/45 | Loss: 0.6674 | IoU: 0.0732\n",
      "    🔍 Pred min/max: -0.9445 ~ 26.1773\n",
      "    📊 Pred after sigmoid: 0.2800 ~ 1.0000\n",
      "  🌀 Batch  16/45 | Loss: 0.6452 | IoU: 0.1231\n",
      "    🔍 Pred min/max: -0.9470 ~ 21.1953\n",
      "    📊 Pred after sigmoid: 0.2795 ~ 1.0000\n",
      "  🌀 Batch  17/45 | Loss: 0.6505 | IoU: 0.0963\n",
      "    🔍 Pred min/max: -0.7304 ~ 37.3047\n",
      "    📊 Pred after sigmoid: 0.3251 ~ 1.0000\n",
      "  🌀 Batch  18/45 | Loss: 0.6372 | IoU: 0.1634\n",
      "    🔍 Pred min/max: -0.9059 ~ 26.6180\n",
      "    📊 Pred after sigmoid: 0.2878 ~ 1.0000\n",
      "  🌀 Batch  19/45 | Loss: 0.6692 | IoU: 0.0771\n",
      "    🔍 Pred min/max: -0.8289 ~ 23.2837\n",
      "    📊 Pred after sigmoid: 0.3039 ~ 1.0000\n",
      "  🌀 Batch  20/45 | Loss: 0.6490 | IoU: 0.0510\n",
      "    🔍 Pred min/max: -0.8160 ~ 39.9067\n",
      "    📊 Pred after sigmoid: 0.3066 ~ 1.0000\n",
      "  🌀 Batch  21/45 | Loss: 0.6620 | IoU: 0.0370\n",
      "    🔍 Pred min/max: -0.9986 ~ 32.9232\n",
      "    📊 Pred after sigmoid: 0.2692 ~ 1.0000\n",
      "  🌀 Batch  22/45 | Loss: 0.6385 | IoU: 0.1064\n",
      "    🔍 Pred min/max: -0.9702 ~ 30.5829\n",
      "    📊 Pred after sigmoid: 0.2748 ~ 1.0000\n",
      "  🌀 Batch  23/45 | Loss: 0.6380 | IoU: 0.1149\n",
      "    🔍 Pred min/max: -0.8224 ~ 40.3782\n",
      "    📊 Pred after sigmoid: 0.3053 ~ 1.0000\n",
      "  🌀 Batch  24/45 | Loss: 0.6388 | IoU: 0.0719\n",
      "    🔍 Pred min/max: -0.7864 ~ 31.6347\n",
      "    📊 Pred after sigmoid: 0.3129 ~ 1.0000\n",
      "  🌀 Batch  25/45 | Loss: 0.6416 | IoU: 0.0650\n",
      "    🔍 Pred min/max: -0.8062 ~ 45.7389\n",
      "    📊 Pred after sigmoid: 0.3087 ~ 1.0000\n",
      "  🌀 Batch  26/45 | Loss: 0.6358 | IoU: 0.0816\n",
      "    🔍 Pred min/max: -0.7786 ~ 39.2665\n",
      "    📊 Pred after sigmoid: 0.3146 ~ 1.0000\n",
      "  🌀 Batch  27/45 | Loss: 0.6310 | IoU: 0.1070\n",
      "    🔍 Pred min/max: -0.7309 ~ 31.2656\n",
      "    📊 Pred after sigmoid: 0.3250 ~ 1.0000\n",
      "  🌀 Batch  28/45 | Loss: 0.6445 | IoU: 0.0742\n",
      "    🔍 Pred min/max: -0.6957 ~ 37.8280\n",
      "    📊 Pred after sigmoid: 0.3328 ~ 1.0000\n",
      "  🌀 Batch  29/45 | Loss: 0.6321 | IoU: 0.1718\n",
      "    🔍 Pred min/max: -0.6497 ~ 41.9249\n",
      "    📊 Pred after sigmoid: 0.3431 ~ 1.0000\n",
      "  🌀 Batch  30/45 | Loss: 0.6454 | IoU: 0.1068\n",
      "    🔍 Pred min/max: -0.8471 ~ 42.6369\n",
      "    📊 Pred after sigmoid: 0.3000 ~ 1.0000\n",
      "  🌀 Batch  31/45 | Loss: 0.6523 | IoU: 0.0468\n",
      "    🔍 Pred min/max: -1.0497 ~ 34.7543\n",
      "    📊 Pred after sigmoid: 0.2593 ~ 1.0000\n",
      "  🌀 Batch  32/45 | Loss: 0.6385 | IoU: 0.0399\n",
      "    🔍 Pred min/max: -0.6482 ~ 40.2531\n",
      "    📊 Pred after sigmoid: 0.3434 ~ 1.0000\n",
      "  🌀 Batch  33/45 | Loss: 0.6298 | IoU: 0.1219\n",
      "    🔍 Pred min/max: -0.6722 ~ 39.2650\n",
      "    📊 Pred after sigmoid: 0.3380 ~ 1.0000\n",
      "  🌀 Batch  34/45 | Loss: 0.6284 | IoU: 0.1053\n",
      "    🔍 Pred min/max: -0.6418 ~ 34.8805\n",
      "    📊 Pred after sigmoid: 0.3448 ~ 1.0000\n",
      "  🌀 Batch  35/45 | Loss: 0.6254 | IoU: 0.1055\n",
      "    🔍 Pred min/max: -0.7594 ~ 41.5003\n",
      "    📊 Pred after sigmoid: 0.3188 ~ 1.0000\n",
      "  🌀 Batch  36/45 | Loss: 0.6393 | IoU: 0.0588\n",
      "    🔍 Pred min/max: -0.5829 ~ 60.6565\n",
      "    📊 Pred after sigmoid: 0.3583 ~ 1.0000\n",
      "  🌀 Batch  37/45 | Loss: 0.6333 | IoU: 0.1095\n",
      "    🔍 Pred min/max: -0.5936 ~ 49.7708\n",
      "    📊 Pred after sigmoid: 0.3558 ~ 1.0000\n",
      "  🌀 Batch  38/45 | Loss: 0.6555 | IoU: 0.0926\n",
      "    🔍 Pred min/max: -0.5473 ~ 35.0749\n",
      "    📊 Pred after sigmoid: 0.3665 ~ 1.0000\n",
      "  🌀 Batch  39/45 | Loss: 0.6295 | IoU: 0.0831\n",
      "    🔍 Pred min/max: -0.6720 ~ 31.6624\n",
      "    📊 Pred after sigmoid: 0.3381 ~ 1.0000\n",
      "  🌀 Batch  40/45 | Loss: 0.6293 | IoU: 0.1401\n",
      "    🔍 Pred min/max: -0.5351 ~ 38.1242\n",
      "    📊 Pred after sigmoid: 0.3693 ~ 1.0000\n",
      "  🌀 Batch  41/45 | Loss: 0.6270 | IoU: 0.1161\n",
      "    🔍 Pred min/max: -0.5849 ~ 38.3668\n",
      "    📊 Pred after sigmoid: 0.3578 ~ 1.0000\n",
      "  🌀 Batch  42/45 | Loss: 0.6242 | IoU: 0.1410\n",
      "    🔍 Pred min/max: -0.5909 ~ 38.2299\n",
      "    📊 Pred after sigmoid: 0.3564 ~ 1.0000\n",
      "  🌀 Batch  43/45 | Loss: 0.6379 | IoU: 0.1240\n",
      "    🔍 Pred min/max: -0.7123 ~ 24.8345\n",
      "    📊 Pred after sigmoid: 0.3291 ~ 1.0000\n",
      "  🌀 Batch  44/45 | Loss: 0.6413 | IoU: 0.1309\n",
      "    🔍 Pred min/max: -0.6598 ~ 33.8778\n",
      "    📊 Pred after sigmoid: 0.3408 ~ 1.0000\n",
      "  🌀 Batch  45/45 | Loss: 0.6167 | IoU: 0.1506\n",
      "    🔍 Pred min/max: -0.6463 ~ 27.9663\n",
      "    📊 Pred after sigmoid: 0.3438 ~ 1.0000\n",
      "✅ Epoch 1/20 完成 | Loss: 0.6501 | IoU: 0.0858 | LR: 0.000500 | 耗时: 680.11s\n",
      "🎉 新的最佳IoU: 0.0858, 模型已保存\n",
      "\n",
      "🚀 Epoch 2/20 开始\n",
      "  🌀 Batch   1/45 | Loss: 0.6290 | IoU: 0.1193\n",
      "    🔍 Pred min/max: -0.6125 ~ 39.0251\n",
      "    📊 Pred after sigmoid: 0.3515 ~ 1.0000\n",
      "  🌀 Batch   2/45 | Loss: 0.6412 | IoU: 0.0674\n",
      "    🔍 Pred min/max: -0.6739 ~ 38.5045\n",
      "    📊 Pred after sigmoid: 0.3376 ~ 1.0000\n",
      "  🌀 Batch   3/45 | Loss: 0.6307 | IoU: 0.1090\n",
      "    🔍 Pred min/max: -0.7747 ~ 31.4593\n",
      "    📊 Pred after sigmoid: 0.3155 ~ 1.0000\n",
      "  🌀 Batch   4/45 | Loss: 0.6475 | IoU: 0.0760\n",
      "    🔍 Pred min/max: -0.5411 ~ 44.3753\n",
      "    📊 Pred after sigmoid: 0.3679 ~ 1.0000\n",
      "  🌀 Batch   5/45 | Loss: 0.6224 | IoU: 0.1331\n",
      "    🔍 Pred min/max: -0.7243 ~ 34.3344\n",
      "    📊 Pred after sigmoid: 0.3264 ~ 1.0000\n",
      "  🌀 Batch   6/45 | Loss: 0.6249 | IoU: 0.0968\n",
      "    🔍 Pred min/max: -0.5182 ~ 39.4580\n",
      "    📊 Pred after sigmoid: 0.3733 ~ 1.0000\n",
      "  🌀 Batch   7/45 | Loss: 0.6343 | IoU: 0.1099\n",
      "    🔍 Pred min/max: -0.4307 ~ 51.7725\n",
      "    📊 Pred after sigmoid: 0.3940 ~ 1.0000\n",
      "  🌀 Batch   8/45 | Loss: 0.6328 | IoU: 0.0831\n",
      "    🔍 Pred min/max: -0.5085 ~ 33.8102\n",
      "    📊 Pred after sigmoid: 0.3755 ~ 1.0000\n",
      "  🌀 Batch   9/45 | Loss: 0.6327 | IoU: 0.0799\n",
      "    🔍 Pred min/max: -0.5078 ~ 49.9777\n",
      "    📊 Pred after sigmoid: 0.3757 ~ 1.0000\n",
      "  🌀 Batch  10/45 | Loss: 0.6436 | IoU: 0.0865\n",
      "    🔍 Pred min/max: -0.5078 ~ 39.2066\n",
      "    📊 Pred after sigmoid: 0.3757 ~ 1.0000\n",
      "  🌀 Batch  11/45 | Loss: 0.6147 | IoU: 0.1599\n",
      "    🔍 Pred min/max: -0.6268 ~ 38.7666\n",
      "    📊 Pred after sigmoid: 0.3482 ~ 1.0000\n",
      "  🌀 Batch  12/45 | Loss: 0.6308 | IoU: 0.1123\n",
      "    🔍 Pred min/max: -0.5835 ~ 40.2487\n",
      "    📊 Pred after sigmoid: 0.3581 ~ 1.0000\n",
      "  🌀 Batch  13/45 | Loss: 0.6348 | IoU: 0.1185\n",
      "    🔍 Pred min/max: -0.6239 ~ 42.4817\n",
      "    📊 Pred after sigmoid: 0.3489 ~ 1.0000\n",
      "  🌀 Batch  14/45 | Loss: 0.6183 | IoU: 0.1269\n",
      "    🔍 Pred min/max: -0.5586 ~ 26.4274\n",
      "    📊 Pred after sigmoid: 0.3639 ~ 1.0000\n",
      "  🌀 Batch  15/45 | Loss: 0.6447 | IoU: 0.0917\n",
      "    🔍 Pred min/max: -0.7323 ~ 39.9182\n",
      "    📊 Pred after sigmoid: 0.3247 ~ 1.0000\n",
      "  🌀 Batch  16/45 | Loss: 0.6449 | IoU: 0.0748\n",
      "    🔍 Pred min/max: -0.6483 ~ 36.2399\n",
      "    📊 Pred after sigmoid: 0.3434 ~ 1.0000\n",
      "  🌀 Batch  17/45 | Loss: 0.6496 | IoU: 0.0890\n",
      "    🔍 Pred min/max: -0.6814 ~ 39.0309\n",
      "    📊 Pred after sigmoid: 0.3359 ~ 1.0000\n",
      "  🌀 Batch  18/45 | Loss: 0.6115 | IoU: 0.1787\n",
      "    🔍 Pred min/max: -0.6012 ~ 30.8396\n",
      "    📊 Pred after sigmoid: 0.3541 ~ 1.0000\n",
      "  🌀 Batch  19/45 | Loss: 0.6345 | IoU: 0.1201\n",
      "    🔍 Pred min/max: -0.6064 ~ 26.6309\n",
      "    📊 Pred after sigmoid: 0.3529 ~ 1.0000\n",
      "  🌀 Batch  20/45 | Loss: 0.6378 | IoU: 0.0931\n",
      "    🔍 Pred min/max: -0.6271 ~ 38.2994\n",
      "    📊 Pred after sigmoid: 0.3482 ~ 1.0000\n",
      "  🌀 Batch  21/45 | Loss: 0.6368 | IoU: 0.1909\n",
      "    🔍 Pred min/max: -0.6090 ~ 33.7338\n",
      "    📊 Pred after sigmoid: 0.3523 ~ 1.0000\n",
      "  🌀 Batch  22/45 | Loss: 0.6353 | IoU: 0.1255\n",
      "    🔍 Pred min/max: -0.6329 ~ 27.6935\n",
      "    📊 Pred after sigmoid: 0.3469 ~ 1.0000\n",
      "  🌀 Batch  23/45 | Loss: 0.6274 | IoU: 0.1285\n",
      "    🔍 Pred min/max: -0.6391 ~ 43.2987\n",
      "    📊 Pred after sigmoid: 0.3454 ~ 1.0000\n",
      "  🌀 Batch  24/45 | Loss: 0.6181 | IoU: 0.1923\n",
      "    🔍 Pred min/max: -0.6978 ~ 29.2077\n",
      "    📊 Pred after sigmoid: 0.3323 ~ 1.0000\n",
      "  🌀 Batch  25/45 | Loss: 0.6244 | IoU: 0.2025\n",
      "    🔍 Pred min/max: -0.7035 ~ 41.4365\n",
      "    📊 Pred after sigmoid: 0.3310 ~ 1.0000\n",
      "  🌀 Batch  26/45 | Loss: 0.6362 | IoU: 0.1172\n",
      "    🔍 Pred min/max: -0.6062 ~ 44.2303\n",
      "    📊 Pred after sigmoid: 0.3529 ~ 1.0000\n",
      "  🌀 Batch  27/45 | Loss: 0.6506 | IoU: 0.1501\n",
      "    🔍 Pred min/max: -0.5657 ~ 44.0727\n",
      "    📊 Pred after sigmoid: 0.3622 ~ 1.0000\n",
      "  🌀 Batch  28/45 | Loss: 0.6180 | IoU: 0.1666\n",
      "    🔍 Pred min/max: -0.5001 ~ 36.3363\n",
      "    📊 Pred after sigmoid: 0.3775 ~ 1.0000\n",
      "  🌀 Batch  29/45 | Loss: 0.6235 | IoU: 0.1126\n",
      "    🔍 Pred min/max: -0.5599 ~ 35.1636\n",
      "    📊 Pred after sigmoid: 0.3636 ~ 1.0000\n",
      "  🌀 Batch  30/45 | Loss: 0.6346 | IoU: 0.0941\n",
      "    🔍 Pred min/max: -0.5193 ~ 41.8754\n",
      "    📊 Pred after sigmoid: 0.3730 ~ 1.0000\n",
      "  🌀 Batch  31/45 | Loss: 0.6205 | IoU: 0.1002\n",
      "    🔍 Pred min/max: -0.5796 ~ 29.2317\n",
      "    📊 Pred after sigmoid: 0.3590 ~ 1.0000\n",
      "  🌀 Batch  32/45 | Loss: 0.6259 | IoU: 0.0900\n",
      "    🔍 Pred min/max: -0.5110 ~ 41.1436\n",
      "    📊 Pred after sigmoid: 0.3749 ~ 1.0000\n",
      "  🌀 Batch  33/45 | Loss: 0.6296 | IoU: 0.1125\n",
      "    🔍 Pred min/max: -0.5716 ~ 28.6606\n",
      "    📊 Pred after sigmoid: 0.3609 ~ 1.0000\n",
      "  🌀 Batch  34/45 | Loss: 0.6346 | IoU: 0.0938\n",
      "    🔍 Pred min/max: -0.5299 ~ 43.0381\n",
      "    📊 Pred after sigmoid: 0.3705 ~ 1.0000\n",
      "  🌀 Batch  35/45 | Loss: 0.6290 | IoU: 0.1402\n",
      "    🔍 Pred min/max: -0.5702 ~ 31.6461\n",
      "    📊 Pred after sigmoid: 0.3612 ~ 1.0000\n",
      "  🌀 Batch  36/45 | Loss: 0.6494 | IoU: 0.0604\n",
      "    🔍 Pred min/max: -0.5633 ~ 50.3162\n",
      "    📊 Pred after sigmoid: 0.3628 ~ 1.0000\n",
      "  🌀 Batch  37/45 | Loss: 0.6238 | IoU: 0.1431\n",
      "    🔍 Pred min/max: -0.6177 ~ 32.6364\n",
      "    📊 Pred after sigmoid: 0.3503 ~ 1.0000\n",
      "  🌀 Batch  38/45 | Loss: 0.6304 | IoU: 0.0597\n",
      "    🔍 Pred min/max: -0.5608 ~ 43.9168\n",
      "    📊 Pred after sigmoid: 0.3634 ~ 1.0000\n",
      "  🌀 Batch  39/45 | Loss: 0.6216 | IoU: 0.1421\n",
      "    🔍 Pred min/max: -0.5532 ~ 44.0253\n",
      "    📊 Pred after sigmoid: 0.3651 ~ 1.0000\n",
      "  🌀 Batch  40/45 | Loss: 0.6197 | IoU: 0.1525\n",
      "    🔍 Pred min/max: -0.5707 ~ 42.0157\n",
      "    📊 Pred after sigmoid: 0.3611 ~ 1.0000\n",
      "  🌀 Batch  41/45 | Loss: 0.6298 | IoU: 0.1200\n",
      "    🔍 Pred min/max: -0.5336 ~ 46.8726\n",
      "    📊 Pred after sigmoid: 0.3697 ~ 1.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 211\u001b[39m\n\u001b[32m    208\u001b[39m loss = combined_loss(preds, masks)\n\u001b[32m    210\u001b[39m optimizer.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m211\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[38;5;66;03m# 更严格的梯度裁剪\u001b[39;00m\n\u001b[32m    214\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=\u001b[32m0.5\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.13/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.13/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.13/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import time\n",
    "\n",
    "# ==== 1. 数据加载部分（保持不变）====\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, image_size=(256, 256)):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.image_size = image_size\n",
    "        self.filenames = sorted(os.listdir(image_dir))\n",
    "\n",
    "        self.transform_img = T.Compose([\n",
    "            T.ToPILImage(),\n",
    "            T.Resize(self.image_size),\n",
    "            T.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_filename = self.filenames[idx]\n",
    "        mask_filename = image_filename.replace(\"RGB_\", \"mask_\")\n",
    "\n",
    "        img_path = os.path.join(self.image_dir, image_filename)\n",
    "        mask_path = os.path.join(self.mask_dir, mask_filename)\n",
    "\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if mask is None:\n",
    "            raise FileNotFoundError(f\"❌ 无法读取 mask 图像: {mask_path}\")\n",
    "\n",
    "        # 转换为二值掩码\n",
    "        mask = (mask > 127).astype(np.float32)\n",
    "\n",
    "        img_tensor = self.transform_img(img)\n",
    "        # 直接处理mask\n",
    "        mask = cv2.resize(mask, self.image_size)\n",
    "        mask_tensor = torch.from_numpy(mask).unsqueeze(0)  # [1, H, W]\n",
    "\n",
    "        return img_tensor, mask_tensor\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "train_dataset = SegmentationDataset(\n",
    "    image_dir=\"data/train/images\",\n",
    "    mask_dir=\"data/train/masks\",\n",
    "    image_size=(256, 256)\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# ==== 2. 改进的U-Net模型（更稳定的版本）====\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        def conv_block(in_c, out_c):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_c, out_c, 3, padding=1),\n",
    "                nn.BatchNorm2d(out_c),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(out_c, out_c, 3, padding=1),\n",
    "                nn.BatchNorm2d(out_c),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout2d(0.1)  # 添加dropout防止过拟合\n",
    "            )\n",
    "\n",
    "        self.enc1 = conv_block(in_channels, 64)\n",
    "        self.enc2 = conv_block(64, 128)\n",
    "        self.enc3 = conv_block(128, 256)\n",
    "        self.enc4 = conv_block(256, 512)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        \n",
    "        # 减少bottleneck的复杂度\n",
    "        self.bottleneck = conv_block(512, 512)  # 从1024改为512\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(512, 512, 2, stride=2)  # 调整通道数\n",
    "        self.dec4 = conv_block(1024, 512)\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
    "        self.dec3 = conv_block(512, 256)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
    "        self.dec2 = conv_block(256, 128)\n",
    "\n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "        self.dec1 = conv_block(128, 64)\n",
    "\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Conv2d(64, out_channels, 1),\n",
    "            nn.BatchNorm2d(out_channels)  # 在最终层添加BN\n",
    "        )\n",
    "        \n",
    "        # 更保守的权重初始化\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.xavier_uniform_(m.weight)  # 使用Xavier初始化\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool(e1))\n",
    "        e3 = self.enc3(self.pool(e2))\n",
    "        e4 = self.enc4(self.pool(e3))\n",
    "        b = self.bottleneck(self.pool(e4))\n",
    "\n",
    "        d4 = self.upconv4(b)\n",
    "        d4 = self.dec4(torch.cat([d4, e4], dim=1))\n",
    "\n",
    "        d3 = self.upconv3(d4)\n",
    "        d3 = self.dec3(torch.cat([d3, e3], dim=1))\n",
    "\n",
    "        d2 = self.upconv2(d3)\n",
    "        d2 = self.dec2(torch.cat([d2, e2], dim=1))\n",
    "\n",
    "        d1 = self.upconv1(d2)\n",
    "        d1 = self.dec1(torch.cat([d1, e1], dim=1))\n",
    "\n",
    "        return self.final(d1)\n",
    "\n",
    "# ==== 3. 更稳定的损失函数 ====\n",
    "def dice_loss(pred, target, smooth=1):\n",
    "    pred = torch.sigmoid(pred)\n",
    "    pred = pred.contiguous().view(-1)\n",
    "    target = target.contiguous().view(-1)\n",
    "    \n",
    "    intersection = (pred * target).sum()\n",
    "    dice = (2. * intersection + smooth) / (pred.sum() + target.sum() + smooth)\n",
    "    return 1 - dice\n",
    "\n",
    "def focal_loss(pred, target, alpha=0.25, gamma=2.0):\n",
    "    \"\"\"Focal Loss for addressing class imbalance\"\"\"\n",
    "    bce_loss = F.binary_cross_entropy_with_logits(pred, target, reduction='none')\n",
    "    pt = torch.exp(-bce_loss)\n",
    "    focal_loss = alpha * (1-pt)**gamma * bce_loss\n",
    "    return focal_loss.mean()\n",
    "\n",
    "# ==== 4. IoU计算函数 ====\n",
    "def compute_iou(pred, target, threshold=0.5):\n",
    "    with torch.no_grad():\n",
    "        pred = torch.sigmoid(pred)\n",
    "        pred_bin = (pred > threshold).float()\n",
    "        target = target.float()\n",
    "\n",
    "        intersection = (pred_bin * target).sum((2, 3))\n",
    "        union = (pred_bin + target).clamp(0, 1).sum((2, 3))\n",
    "        \n",
    "        iou = torch.where(union == 0, torch.tensor(1.0, device=union.device), \n",
    "                         intersection / (union + 1e-8))\n",
    "        return iou.mean().item()\n",
    "\n",
    "# ==== 5. 训练代码 ====\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"使用设备: {device}\")\n",
    "\n",
    "model = UNet().to(device)\n",
    "\n",
    "# 使用更稳定的损失函数和优化器设置\n",
    "def combined_loss(pred, target):\n",
    "    bce = F.binary_cross_entropy_with_logits(pred, target, pos_weight=torch.tensor(3.0).to(device))\n",
    "    dice = dice_loss(pred, target)\n",
    "    focal = focal_loss(pred, target)\n",
    "    return 0.5 * bce + 0.3 * dice + 0.2 * focal\n",
    "\n",
    "# 更保守的优化器设置\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-5)\n",
    "\n",
    "# 更温和的学习率调度\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=2\n",
    ")\n",
    "\n",
    "# 添加早停机制\n",
    "best_iou = 0\n",
    "patience_counter = 0\n",
    "patience = 5\n",
    "\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\n🚀 Epoch {epoch+1}/{epochs} 开始\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    model.train()\n",
    "    total_loss, total_iou = 0, 0\n",
    "\n",
    "    for batch_idx, (imgs, masks) in enumerate(train_loader):\n",
    "        imgs = imgs.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        preds = model(imgs)\n",
    "        loss = combined_loss(preds, masks)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # 更严格的梯度裁剪\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_iou = compute_iou(preds, masks)\n",
    "        total_loss += loss.item()\n",
    "        total_iou += batch_iou\n",
    "\n",
    "        # 每10个batch打印一次\n",
    "        if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == len(train_loader):\n",
    "            print(f\"  🌀 Batch {batch_idx+1:>3}/{len(train_loader)} | Loss: {loss.item():.4f} | IoU: {batch_iou:.4f}\")\n",
    "            print(f\"    🔍 Pred min/max: {preds.min().item():.4f} ~ {preds.max().item():.4f}\")\n",
    "            print(f\"    📊 Pred after sigmoid: {torch.sigmoid(preds).min().item():.4f} ~ {torch.sigmoid(preds).max().item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    avg_iou = total_iou / len(train_loader)\n",
    "    elapsed = time.time() - start_time\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    print(f\"✅ Epoch {epoch+1}/{epochs} 完成 | Loss: {avg_loss:.4f} | IoU: {avg_iou:.4f} | LR: {current_lr:.6f} | 耗时: {elapsed:.2f}s\")\n",
    "\n",
    "    # 学习率调度\n",
    "    scheduler.step(avg_loss)\n",
    "    \n",
    "    # 早停检查\n",
    "    if avg_iou > best_iou:\n",
    "        best_iou = avg_iou\n",
    "        patience_counter = 0\n",
    "        # 保存最佳模型\n",
    "        torch.save(model.state_dict(), 'best_unet_model.pth')\n",
    "        print(f\"🎉 新的最佳IoU: {best_iou:.4f}, 模型已保存\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"🛑 早停触发，最佳IoU: {best_iou:.4f}\")\n",
    "            break\n",
    "\n",
    "print(f\"\\n🎯 训练完成！最佳IoU: {best_iou:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
