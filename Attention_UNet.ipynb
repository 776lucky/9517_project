{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/776lucky/9517_project.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def save_prediction_images(model, dataset, epoch, indices=[0, 1, 2], save_dir=\"predictions\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    model.eval()\n",
    "\n",
    "    for idx in indices:\n",
    "        img, mask = dataset[idx]\n",
    "        with torch.no_grad():\n",
    "            pred = model(img.unsqueeze(0).to(DEVICE))\n",
    "            pred = torch.sigmoid(pred).squeeze().cpu().numpy()\n",
    "\n",
    "        img_rgb = img[:3].permute(1, 2, 0).numpy()\n",
    "        mask_np = mask.squeeze().numpy()\n",
    "\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
    "        axs[0].imshow(img_rgb); axs[0].set_title(\"RGB Image\")\n",
    "        axs[1].imshow(mask_np, cmap='gray'); axs[1].set_title(\"Ground Truth\")\n",
    "        axs[2].imshow(pred, cmap='gray'); axs[2].set_title(\"Prediction\")\n",
    "        for ax in axs: ax.axis(\"off\")\n",
    "\n",
    "        save_path = os.path.join(save_dir, f\"epoch_{epoch}_idx_{idx}.png\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "        print(f\"üì∏ È¢ÑÊµãÂõæÂ∑≤‰øùÂ≠òËá≥ {save_path}\")\n",
    "\n",
    "\n",
    "\n",
    "# ===== Êï∞ÊçÆÈ¢ÑÂ§ÑÁêÜÈÉ®ÂàÜ =====\n",
    "def build_file_lists(rgb_dir, nrg_dir, train_ratio=0.8):\n",
    "    rgb_files = [f.replace(\"RGB_\", \"\") for f in os.listdir(rgb_dir) if f.startswith(\"RGB_\")]\n",
    "    nrg_files = [f.replace(\"NRG_\", \"\") for f in os.listdir(nrg_dir) if f.startswith(\"NRG_\")]\n",
    "    common_ids = sorted(set(rgb_files) & set(nrg_files))\n",
    "    print(f\"‚úÖ ÊâæÂà∞ {len(common_ids)} Âº† RGB Âíå NRG ÂØπÂ∫îÂõæÂÉè\")\n",
    "\n",
    "    rgb_filenames = [\"RGB_\" + fid for fid in common_ids]\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(rgb_filenames)\n",
    "    n_train = int(train_ratio * len(rgb_filenames))\n",
    "    return rgb_filenames[:n_train], rgb_filenames[n_train:]\n",
    "\n",
    "class FourChannelSegmentationDataset(Dataset):\n",
    "    def __init__(self, rgb_dir, nrg_dir, mask_dir, file_list, image_size=(256, 256)):\n",
    "        self.rgb_dir = rgb_dir\n",
    "        self.nrg_dir = nrg_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.file_list = file_list\n",
    "        self.image_size = image_size\n",
    "\n",
    "        self.transform_img = T.Compose([\n",
    "            T.ToPILImage(),\n",
    "            T.Resize(self.image_size),\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.file_list[idx]\n",
    "        img_id = filename.replace(\"RGB_\", \"\")\n",
    "        rgb_path = os.path.join(self.rgb_dir, filename)\n",
    "        nrg_path = os.path.join(self.nrg_dir, \"NRG_\" + img_id)\n",
    "        mask_path = os.path.join(self.mask_dir, \"mask_\" + img_id)\n",
    "\n",
    "        rgb = cv2.imread(rgb_path)\n",
    "        rgb = cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB)\n",
    "        nrg = cv2.imread(nrg_path)\n",
    "        nir_2d = nrg[:, :, 0]\n",
    "        img_4ch = np.concatenate((rgb, nir_2d[..., None]), axis=-1)\n",
    "\n",
    "        rgb_tensor = self.transform_img(img_4ch[:, :, :3])\n",
    "        nir_tensor = self.transform_img(nir_2d)\n",
    "        img_tensor = torch.cat([rgb_tensor, nir_tensor], dim=0)\n",
    "\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        mask = cv2.resize(mask, self.image_size)\n",
    "        mask = (mask > 127).astype(np.float32)\n",
    "        mask_tensor = torch.from_numpy(mask).unsqueeze(0)\n",
    "\n",
    "        return img_tensor, mask_tensor\n",
    "\n",
    "def get_datasets(base_dir=\"9517_project/USA_segmentation\", image_size=(256, 256)):\n",
    "    rgb_dir = os.path.join(base_dir, \"RGB_images\")\n",
    "    nrg_dir = os.path.join(base_dir, \"NRG_images\")\n",
    "    mask_dir = os.path.join(base_dir, \"masks\")\n",
    "    train_files, test_files = build_file_lists(rgb_dir, nrg_dir)\n",
    "    train_dataset = FourChannelSegmentationDataset(rgb_dir, nrg_dir, mask_dir, train_files, image_size)\n",
    "    test_dataset = FourChannelSegmentationDataset(rgb_dir, nrg_dir, mask_dir, test_files, image_size)\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "# ===== Ê®°ÂûãËÆ≠ÁªÉÈÉ®ÂàÜ =====\n",
    "# === 1. ÂèÇÊï∞ËÆæÁΩÆ ===\n",
    "IMG_SIZE = (256, 256)\n",
    "IN_CHANNELS = 4\n",
    "OUT_CHANNELS = 1\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 3e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "PATIENCE = 10\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === 2. Âä†ËΩΩÊï∞ÊçÆ ===\n",
    "train_dataset, test_dataset = get_datasets(\"9517_project/USA_segmentation\", image_size=IMG_SIZE)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, F_g, F_l, F_int):\n",
    "        super().__init__()\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, kernel_size=1),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, kernel_size=1),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, kernel_size=1),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "        psi = self.relu(g1 + x1)\n",
    "        psi = self.psi(psi)\n",
    "        return x * psi\n",
    "\n",
    "\n",
    "class AttentionUNet(nn.Module):\n",
    "    def __init__(self, in_channels=4, out_channels=1):\n",
    "        super().__init__()\n",
    "\n",
    "        def conv_block(in_c, out_c):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_c, out_c, 3, padding=1),\n",
    "                nn.BatchNorm2d(out_c),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(out_c, out_c, 3, padding=1),\n",
    "                nn.BatchNorm2d(out_c),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout2d(0.1)\n",
    "            )\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "        self.enc1 = conv_block(in_channels, 64)\n",
    "        self.enc2 = conv_block(64, 128)\n",
    "        self.enc3 = conv_block(128, 256)\n",
    "        self.enc4 = conv_block(256, 512)\n",
    "        self.bottleneck = conv_block(512, 512)\n",
    "\n",
    "        self.att4 = AttentionBlock(F_g=512, F_l=512, F_int=256)\n",
    "        self.att3 = AttentionBlock(F_g=256, F_l=256, F_int=128)\n",
    "        self.att2 = AttentionBlock(F_g=128, F_l=128, F_int=64)\n",
    "        self.att1 = AttentionBlock(F_g=64,  F_l=64,  F_int=32)\n",
    "\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(512, 512, 2, stride=2)\n",
    "        self.dec4 = conv_block(1024, 512)\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
    "        self.dec3 = conv_block(512, 256)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
    "        self.dec2 = conv_block(256, 128)\n",
    "\n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "        self.dec1 = conv_block(128, 64)\n",
    "\n",
    "        self.final = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "        nn.init.constant_(self.final.bias, -2.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool(e1))\n",
    "        e3 = self.enc3(self.pool(e2))\n",
    "        e4 = self.enc4(self.pool(e3))\n",
    "        b = self.bottleneck(self.pool(e4))\n",
    "\n",
    "        d4 = self.upconv4(b)\n",
    "        e4 = self.att4(g=d4, x=e4)\n",
    "        d4 = self.dec4(torch.cat([d4, e4], dim=1))\n",
    "\n",
    "        d3 = self.upconv3(d4)\n",
    "        e3 = self.att3(g=d3, x=e3)\n",
    "        d3 = self.dec3(torch.cat([d3, e3], dim=1))\n",
    "\n",
    "        d2 = self.upconv2(d3)\n",
    "        e2 = self.att2(g=d2, x=e2)\n",
    "        d2 = self.dec2(torch.cat([d2, e2], dim=1))\n",
    "\n",
    "        d1 = self.upconv1(d2)\n",
    "        e1 = self.att1(g=d1, x=e1)\n",
    "        d1 = self.dec1(torch.cat([d1, e1], dim=1))\n",
    "\n",
    "        return self.final(d1)\n",
    "\n",
    "\n",
    "# === 4. ÂàùÂßãÂåñÊ®°Âûã ===\n",
    "model = AttentionUNet().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)\n",
    "\n",
    "# === 5. ÊçüÂ§±ÂáΩÊï∞‰∏é IoU ===\n",
    "def focal_loss(pred, target, alpha=0.75, gamma=2.5):\n",
    "    bce = F.binary_cross_entropy_with_logits(pred, target, reduction='none')\n",
    "    prob = torch.sigmoid(pred)\n",
    "    loss = alpha * (1 - prob)**gamma * bce\n",
    "    return loss.mean()\n",
    "\n",
    "def tversky_loss(pred, target, alpha=0.7, beta=0.3, smooth=1e-6):\n",
    "    pred = torch.sigmoid(pred)\n",
    "    TP = (pred * target).sum((1,2,3))\n",
    "    FP = ((1 - target) * pred).sum((1,2,3))\n",
    "    FN = (target * (1 - pred)).sum((1,2,3))\n",
    "    return 1 - ((TP + smooth) / (TP + alpha * FP + beta * FN + smooth)).mean()\n",
    "\n",
    "\n",
    "def dice_loss(pred, target, smooth=1e-6):\n",
    "    pred = torch.sigmoid(pred)\n",
    "    intersection = (pred * target).sum((1,2,3))\n",
    "    union = pred.sum((1,2,3)) + target.sum((1,2,3))\n",
    "    return 1 - ((2. * intersection + smooth) / (union + smooth)).mean()\n",
    "\n",
    "def combined_loss(pred, target):\n",
    "    return 0.5 * focal_loss(pred, target, alpha=0.85, gamma=3.0) + 0.5 * tversky_loss(pred, target)\n",
    "\n",
    "\n",
    "def compute_iou(pred, target, threshold=0.3):\n",
    "    with torch.no_grad():\n",
    "        pred_bin = (torch.sigmoid(pred) > threshold).float()\n",
    "        intersection = (pred_bin * target).sum((2, 3))\n",
    "        union = (pred_bin + target).clamp(0, 1).sum((2, 3))\n",
    "        return (intersection / (union + 1e-8)).mean().item()\n",
    "\n",
    "def visualize_prediction(model, dataset, index=0):\n",
    "    model.eval()\n",
    "    img, mask = dataset[index]\n",
    "    with torch.no_grad():\n",
    "        pred = model(img.unsqueeze(0).to(DEVICE))\n",
    "        pred = torch.sigmoid(pred).squeeze().cpu().numpy()\n",
    "\n",
    "    img_rgb = img[:3].permute(1, 2, 0).numpy()\n",
    "    mask_np = mask.squeeze().numpy()\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 3, 1); plt.imshow(img_rgb); plt.title(\"RGB Image\")\n",
    "    plt.subplot(1, 3, 2); plt.imshow(mask_np, cmap='gray'); plt.title(\"Ground Truth\")\n",
    "    plt.subplot(1, 3, 3); plt.imshow(pred, cmap='gray'); plt.title(\"Prediction\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "# === 6. Ê®°ÂûãËÆ≠ÁªÉ‰∏ªÂæ™ÁéØ ===\n",
    "best_iou = 0\n",
    "patience_counter = 0\n",
    "\n",
    "print(\"üöÄ ÂºÄÂßãËÆ≠ÁªÉ U-Net\")\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss, total_iou = 0, 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for batch_idx, (imgs, masks) in enumerate(train_loader):\n",
    "        imgs, masks = imgs.to(DEVICE), masks.to(DEVICE)\n",
    "        preds = model(imgs)\n",
    "        loss = combined_loss(preds, masks)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_iou = compute_iou(preds, masks)\n",
    "        total_loss += loss.item()\n",
    "        total_iou += batch_iou\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    avg_iou = total_iou / len(train_loader)\n",
    "    current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    print(f\"‚úÖ Epoch {epoch+1}/{EPOCHS} ÂÆåÊàê | Loss: {avg_loss:.4f} | IoU: {avg_iou:.4f} | LR: {current_lr:.6f} | ËÄóÊó∂: {elapsed:.2f}s\")\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        save_prediction_images(model, train_dataset, epoch + 1, indices=[0, 1, 2])\n",
    "        visualize_prediction(model, train_dataset, index=0)  # ÊòæÁ§∫Ê†∑Êú¨\n",
    "\n",
    "    scheduler.step(avg_iou)\n",
    "\n",
    "    if avg_iou > best_iou:\n",
    "        best_iou = avg_iou\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), \"best_unet_model.pth\")\n",
    "        print(f\"üéâ ÊúÄ‰Ω≥IoUÊõ¥Êñ∞‰∏∫ {best_iou:.4f}ÔºåÊ®°ÂûãÂ∑≤‰øùÂ≠ò\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"üïí Êú™ÊèêÂçáÔºà{patience_counter}/{PATIENCE}ÔºâÔºåÁ≠âÂæÖ‰∏≠...\")\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f\"üõë Early stopping Ëß¶Âèë„ÄÇÊúÄ‰Ω≥ IoU = {best_iou:.4f}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AttentionUNet().to(DEVICE)  # Êàñ UNet() Áúã‰Ω†‰ΩøÁî®Âì™‰∏™\n",
    "model.load_state_dict(torch.load(\"best_unet_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "total_iou = 0\n",
    "\n",
    "for idx, (img, mask) in enumerate(test_dataset):\n",
    "    img = img.unsqueeze(0).to(DEVICE)\n",
    "    mask = mask.unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = model(img)\n",
    "        iou = compute_iou(pred, mask, threshold=0.3)  # ÂèØË∞É threshold\n",
    "        total_iou += iou\n",
    "\n",
    "    print(f\"üìç Test Sample {idx:03d} | IoU: {iou:.4f}\")\n",
    "\n",
    "avg_iou = total_iou / len(test_dataset)\n",
    "print(f\"\\n‚úÖ ÊµãËØïÈõÜÂπ≥Âùá IoU: {avg_iou:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
